### Cache line

我们需要了解一个术语 Cache Line。缓存基本上来说就是把后面的数据加载到离自己近的地方，对于CPU来说，它是不会一个字节一个字节的加载的，因为这非常没有效率，一般来说都是要一块一块的加载的，对于这样的一块一块的数据单位，术语叫“Cache Line”，一般来说，一个主流的CPU的Cache Line 是 64 Bytes，64Bytes也就是16个32位的整型，这就是CPU从内存中捞数据上来的最小数据单位。

比如：Cache Line是最小单位（64Bytes），所以先把Cache分布多个Cache Line，比如：L1有32KB，那么，32KB/64B = 512 个 Cache Line。

一方面，缓存需要把内存里的数据放到放进来，英文叫 CPU Associativity。Cache的数据放置的策略决定了内存中的数据块会拷贝到CPU Cache中的哪个位置上，因为Cache的大小远远小于内存，所以，需要有一种地址关联的算法，能够让内存中的数据可以被映射到Cache中来。这个有点像内存地址从逻辑地址向物理地址映射的方法，但不完全一样。

基本上来说，我们会有如下的一些方法。

- 一种方法是，任何一个内存地址的数据可以被缓存在任何一个Cache     Line里，这种方法是最灵活的，但是，如果我们要知道一个内存是否存在于Cache中，我们就需要进行O(n)复杂度的Cache遍历，这是很没有效率的。
- 另一种方法，为了降低缓存搜索算法，我们需要使用像Hash     Table这样的数据结构，最简单的hash     table就是做“求模运算”，比如：我们的L1 Cache有512个Cache Line，那么，公式：（内存地址     mod 512）* 64 就可以直接找到所在的Cache地址的偏移了。但是，这样的方式需要我们的程序对内存地址的访问要非常地平均，不然冲突就会非常严重。这成了一种非常理想的情况了。
- 为了避免上述的两种方案的问题，于是就要容忍一定的hash冲突，也就出现了     N-Way 关联。也就是把连续的N个Cache Line绑成一组，然后，先把找到相关的组，然后再在这个组内找到相关的Cache     Line。这叫 Set Associativity。如下图所示。

![image.png](https://i.loli.net/2020/10/08/jSOozUT8kAbp4Ns.png)

对于 N-Way 组关联，Intel 大多数处理器的L1 Cache都是32KB，8-Way 组相联，Cache Line 是64 Bytes。这意味着，

- 32KB的可以分成，32KB     / 64 = 512 条 Cache Line。
- 因为有8     Way，于是会每一Way 有512 / 8 = 64 条 Cache Line。
- 于是每一路就有     64 x 64 = 4096 Byts 的内存。

为了方便索引内存地址，

- **Tag**：每条 Cache Line 前都会有一个独立分配的     24 bits来存的 tag，其就是内存地址的前24bits
- **Index**：内存地址后续的6个bits则是在这一Way的是Cache     Line 索引，2^6 = 64 刚好可以索引64条Cache     Line
- **Offset**：再往后的6bits用于表示在Cache     Line 里的偏移量

如下图所示：

当拿到一个内存地址的时候，先拿出中间的 6bits 来，找到是哪组。

![image.png](https://i.loli.net/2020/10/08/26OSzTEwXCbdcLB.png)

然后，在这一个8组的cache line中，再进行O(n) n=8 的遍历，主是要匹配前24bits的tag。如果匹配中了，就算命中，如果没有匹配到，那就是cache miss，如果是读操作，就需要进向后面的缓存进行访问了。L2/L3同样是这样的算法。而淘汰算法有两种，一种是随机一种是LRU。现在一般都是以LRU的算法（通过增加一个访问计数器来实现）

![image.png](https://i.loli.net/2020/10/08/KMBaktHnofupyCR.png)

这也意味着：

- L1 Cache 可映射 36bits 的内存地址，一共 2^36 = 64GB的内存
- 当CPU要访问一个内存的时候，通过这个内存中间的6bits     定位是哪个set，通过前24bits 定位相应的Cache Line。
- 就像一个hash Table的数据结构一样，先是O(1)的索引，然后进入冲突搜索。
- 因为中间的6bits 决定了一个同一个set，所以，对于一段连续的内存来说，每隔4096的内存会被放在同一个组内，导致缓存冲突。

此外，当有数据没有命中缓存的时候，CPU就会以最小为Cache Line的单元向内存更新数据。当然，CPU并不一定只是更新64Bytes，因为访问主存实在是太慢了，所以，一般都会多更新一些。好的CPU会有一些预测的技术，如果找到一种pattern的话，就会预先加载更多的内存，包括指令也可以预加载。这叫 Prefetching 技术 。比如，你在for-loop访问一个连续的数组，你的步长是一个固定的数，内存就可以做到prefetching。



### 缓存的一致性

对于主流的CPU来说，缓存的写操作基本上是两种策略

- 一种是Write     Back，写操作只要在cache上，然后再flush到内存上。
- 一种是Write     Through，写操作同时写到cache和内存上。

为了提高写的性能，一般来说，主流的CPU采用的是Write Back的策略，因为直接写内存实在是太慢了。

好了，现在问题来了，如果有一个数据 x 在 CPU 第0核的缓存上被更新了，那么其它CPU核上对于这个数据 x 的值也要被更新，这就是缓存一致性的问题。一般来说，在CPU硬件上，会有两种方法来解决这个问题。

- **Directory** **协议**。这种方法的典型实现是要设计一个集中式控制器，它是主存储器控制器的一部分。其中有一个目录存储在主存储器中，其中包含有关各种本地缓存内容的全局状态信息。当单个CPU     Cache 发出读写请求时，这个集中式控制器会检查并发出必要的命令，以在主存和CPU     Cache之间或在CPU Cache自身之间进行数据同步和传输。
- **Snoopy** **协议**。这种协议更像是一种数据通知的总线型的技术。CPU     Cache通过这个协议可以识别其它Cache上的数据状态。如果有数据共享的话，可以通过广播机制将共享数据的状态通知给其它CPU     Cache。这个协议要求每个CPU Cache 都可以**“窥探”**数据事件的通知并做出相应的反应。如下图所示，有一个Snoopy     Bus的总线。

![image.png](https://i.loli.net/2020/10/08/JDMBGOoaVP6i382.png)

现在的CPU基本用的都是总线式,

![img](https://pic4.zhimg.com/80/v2-d36713b8be816c3fc9cad901bc9c0f3b_1440w.jpg)如图所示，一个变量（一个内存位置）其实可以被多个Cache所共享。那么，当我们需要修改这个变量的时候，Cache要如何保持一致呢？

理想情况下，原子地修改多个Cache，但多个CPU之间往往通过总线进行通信，不可能同时修改多个；所以其实要制造一种假象，看起来是原子地修改多个Cache，也就是让Cache看起来是强一致的。

![img](https://pic1.zhimg.com/80/v2-f75bd8f7006006b2fb8b587a81f68d04_1440w.jpg)

基于总线通信去实现Cache的强一致，这个问题比较明确，目前用的比较多的应该是MESI协议，或者是一些优化的协议。基本思想是这样子的：一个Cache加载一个变量的时候，是Exclusive状态，当这个变量被第二个Cache加载，更改状态为Shared；这时候一个CPU要修改变量， 就把状态改为Modified，并且Invalidate其他的Cache，其他的Cache再去读这个变量，达到一致。MESI协议大致是这样子，但是状态转换要比这个复杂的多。

![img](https://pic2.zhimg.com/80/v2-f11ec6e6b0f9d77f3653a42045bbb911_1440w.jpg)

看起来很美好的MESI协议，其实有一些问题。比如说，修改变量的时候，要发送一些Invalidate给远程的CPU，等到远程CPU返回一个ACK，才能进行下一步。 这一过程中如果远程的CPU比较繁忙，甚至会带来更大的延迟。并且如果有内存访问，会带来几百个周期的延迟。

那么有没有优化手段，能够并行访问内存？或者对内存操作乱序执行？

![img](https://pic2.zhimg.com/80/v2-be68901e3f48768d3c911c2c36bdf109_1440w.jpg)

但是有一个问题我们显然不能忽视，例如这里的例子。程序的正确性依赖了一个假定，x = 1024 这个语句要先于 flag = true 执行。如果这个顺序被破坏了，那么后面的断言就会出错了。

![img](https://pic3.zhimg.com/80/v2-890f9aeb259b5b772f33bfa5cb8a3472_1440w.jpg)

不过先不管这么多，我们先优化一下性能再说。

这里用了一个称之为store buffer的结构，来对store操作进行优化。

![img](https://pic4.zhimg.com/80/v2-69bcbdf56867531799fc699544a1192b_1440w.jpg)

以及Invalidate Queue的结构，可以缓解大量的Invalidate Message的问题。

就Store操作来说，这两个结构所带来的效果就是，不需要等到Cache同步到所有CPU之后Store操作才返回，可能是写了本地的Store buffer就返回，或者invalidate message发到远程CPU，但是远程CPU还没有执行，本地的Store操作也可以返回。显然，这两个结果对于延迟的优化是十分明显的。

![img](https://pic4.zhimg.com/80/v2-1182cc5a55b6d59e2b5c29ab7329fe2b_1440w.jpg)

但是这两个结构肯定会带来乱序的问题，也就是说，本地的Store操作返回了，但其实远程还不能读到，还没有生效，而后面的操作先执行了。如何解决乱序的问题？

CPU通常提供了内存屏障指令，来解决这样的问题。读屏障，清空本地的invalidate queue，保证之前的所有load都已经生效；写屏障，清空本地的store buffer，使得之前的所有store操作都生效。

这里也解释了让我困惑许久的问题，为什么原子变量load和store要配对使用，仅仅store为什么还不够。

![img](https://pic2.zhimg.com/80/v2-e56159f3333000d8cece6bacbaf5e725_1440w.jpg)

使用这两个内存屏障，我们可以对之前的代码做一些修改，来保证正确。

![img](https://pic1.zhimg.com/80/v2-7a1f85bc13f2b526ea3175d3bca6156c_1440w.jpg)

但是，处理器领域其实还有很多的优化手段，流水线执行、乱序执行、预测执行等等，各种我听过和没听过的优化，他们对顺序的影响又是怎样的？以及，我们所说的内存屏障，能否通过形式化方法证明其正确性呢？而不是拘泥于某一个处理器的实现细节，去讨论程序的正确与否，否则这样给程序员带来的心智负担就太重了。

![img](https://pic1.zhimg.com/80/v2-f5299306348a21ae5a740780ec7e0a3c_1440w.jpg)

这里列举了一下主流的CPU架构对指令重排的约定。可以关注一下我们用的比较多的x86，它其实只有一种重排，Store-Load，其实这种称之为Total Store Ordering。除此之外还有很多中处理器，对于重排的定义都不太一样，最弱的Alpha，所有重排都会发生，这种情况下我们的代码要怎么写呢？

![img](https://pic4.zhimg.com/80/v2-12d1f2e228e5e63d8cc03cef9e136227_1440w.jpg)

因此便有了内存模型（Memory Model），它是系统和程序员之间的规范，它规定了存储器访问的行为，并影响到了性能。并且，Memory Model有多层，处理器规定、编译器规定、高级语。对于高级语言来说， 它通常需要支持跨平台，也就是说它会基于各种不同的内存模型，但是又要提供给程序员一个统一的内存模型，可以理解为一个适配器的角色。

Memory Model由Instruction Reordering和Store Atomicity来定义。总是就是每种模型对于乱序的定义不太一样。

![img](https://pic3.zhimg.com/80/v2-ec0ed44dead2c4537dca98abe47629e2_1440w.jpg)

这里其实就有了Memory Consistency的概念，与Cache Coherence不同的是，Memory Consistency关注的是多个变量，而非单个变量；Memory Model是多处理器和编译器优化导致存储器操作被多个处理器观察到的顺序不一致的问题，而Cache Coherence对程序员来说是透明的。

从强到弱，分别是Sequential Consistency，Weak Consistency等等。

![img](https://pic2.zhimg.com/80/v2-165d5c3dc74907ebdf553d2db18b2e05_1440w.jpg)

在CPU这个层面，我们往往不讨论Linearizability，但是在编程的时候通常会把它作为衡量程序正确性的标志，所谓的线程安全/并发安全，通常指的也就是Linearizability。

补充：并发安全和线性一致并不等价，也有非线性一致但并发安全的对象，比如静态一致顺序一致，在某些特殊的场景下不需要保证线性一致也能保证安全。下图引用自《多处理器编程艺术》3.5.2节。

![img](https://pic3.zhimg.com/80/v2-7df773ed4c228844928bedaff2d5506a_1440w.jpg)



![img](https://pic2.zhimg.com/80/v2-b71ebcf8af3e3371d198c2e485250bed_1440w.jpg)

对于Memory Order来说，最重要的是Sequential Consistency。它的意思是，每个线程按照程序次序执行，多个线程的执行结果，和将所有操作顺序执行的结果一样。换句话说，将所有线程的操作按照某一个顺序依次执行，结果和原来一样，但这个顺序未必是时间顺序。

硬件通常也不能保证SC，因为这样性能太差了；但是高级语言通过一些同步手段，通常能保证SC。

现实中的处理器，往往是Relaxed Consistency的，存在着各种乱序，实现为Release consisteny/Processor consistency/Weak Ordering等各种内存模型。

![img](https://pic4.zhimg.com/80/v2-cb7d30bb2a4d516c09c0b33ecc5b479b_1440w.jpg)

总而言之，多处理器编程所要解决的问题，就是基于一个Relaxed Consistency的处理器，通过编程语言和CPU提供的能保证Sequential Consistency的同步原语，最终实现Linearizability。



### Java中的缓存知识