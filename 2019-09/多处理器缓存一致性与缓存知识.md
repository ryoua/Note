### Cache line

我们需要了解一个术语 Cache Line。缓存基本上来说就是把后面的数据加载到离自己近的地方，对于CPU来说，它是不会一个字节一个字节的加载的，因为这非常没有效率，一般来说都是要一块一块的加载的，对于这样的一块一块的数据单位，术语叫“Cache Line”，一般来说，一个主流的CPU的Cache Line 是 64 Bytes，64Bytes也就是16个32位的整型，这就是CPU从内存中捞数据上来的最小数据单位。

比如：Cache Line是最小单位（64Bytes），所以先把Cache分布多个Cache Line，比如：L1有32KB，那么，32KB/64B = 512 个 Cache Line。

一方面，缓存需要把内存里的数据放到放进来，英文叫 CPU Associativity。Cache的数据放置的策略决定了内存中的数据块会拷贝到CPU Cache中的哪个位置上，因为Cache的大小远远小于内存，所以，需要有一种地址关联的算法，能够让内存中的数据可以被映射到Cache中来。这个有点像内存地址从逻辑地址向物理地址映射的方法，但不完全一样。

基本上来说，我们会有如下的一些方法。

- 一种方法是，任何一个内存地址的数据可以被缓存在任何一个Cache     Line里，这种方法是最灵活的，但是，如果我们要知道一个内存是否存在于Cache中，我们就需要进行O(n)复杂度的Cache遍历，这是很没有效率的。
- 另一种方法，为了降低缓存搜索算法，我们需要使用像Hash     Table这样的数据结构，最简单的hash     table就是做“求模运算”，比如：我们的L1 Cache有512个Cache Line，那么，公式：（内存地址     mod 512）* 64 就可以直接找到所在的Cache地址的偏移了。但是，这样的方式需要我们的程序对内存地址的访问要非常地平均，不然冲突就会非常严重。这成了一种非常理想的情况了。
- 为了避免上述的两种方案的问题，于是就要容忍一定的hash冲突，也就出现了     N-Way 关联。也就是把连续的N个Cache Line绑成一组，然后，先把找到相关的组，然后再在这个组内找到相关的Cache     Line。这叫 Set Associativity。如下图所示。

![image.png](https://i.loli.net/2020/10/08/jSOozUT8kAbp4Ns.png)

对于 N-Way 组关联，Intel 大多数处理器的L1 Cache都是32KB，8-Way 组相联，Cache Line 是64 Bytes。这意味着，

- 32KB的可以分成，32KB     / 64 = 512 条 Cache Line。
- 因为有8     Way，于是会每一Way 有512 / 8 = 64 条 Cache Line。
- 于是每一路就有     64 x 64 = 4096 Byts 的内存。

为了方便索引内存地址，

- **Tag**：每条 Cache Line 前都会有一个独立分配的     24 bits来存的 tag，其就是内存地址的前24bits
- **Index**：内存地址后续的6个bits则是在这一Way的是Cache     Line 索引，2^6 = 64 刚好可以索引64条Cache     Line
- **Offset**：再往后的6bits用于表示在Cache     Line 里的偏移量

如下图所示：

当拿到一个内存地址的时候，先拿出中间的 6bits 来，找到是哪组。

![image.png](https://i.loli.net/2020/10/08/26OSzTEwXCbdcLB.png)

然后，在这一个8组的cache line中，再进行O(n) n=8 的遍历，主是要匹配前24bits的tag。如果匹配中了，就算命中，如果没有匹配到，那就是cache miss，如果是读操作，就需要进向后面的缓存进行访问了。L2/L3同样是这样的算法。而淘汰算法有两种，一种是随机一种是LRU。现在一般都是以LRU的算法（通过增加一个访问计数器来实现）

![image.png](https://i.loli.net/2020/10/08/KMBaktHnofupyCR.png)

这也意味着：

- L1 Cache 可映射 36bits 的内存地址，一共 2^36 = 64GB的内存
- 当CPU要访问一个内存的时候，通过这个内存中间的6bits     定位是哪个set，通过前24bits 定位相应的Cache Line。
- 就像一个hash Table的数据结构一样，先是O(1)的索引，然后进入冲突搜索。
- 因为中间的6bits 决定了一个同一个set，所以，对于一段连续的内存来说，每隔4096的内存会被放在同一个组内，导致缓存冲突。

此外，当有数据没有命中缓存的时候，CPU就会以最小为Cache Line的单元向内存更新数据。当然，CPU并不一定只是更新64Bytes，因为访问主存实在是太慢了，所以，一般都会多更新一些。好的CPU会有一些预测的技术，如果找到一种pattern的话，就会预先加载更多的内存，包括指令也可以预加载。这叫 Prefetching 技术 。比如，你在for-loop访问一个连续的数组，你的步长是一个固定的数，内存就可以做到prefetching。



### 缓存的一致性

对于主流的CPU来说，缓存的写操作基本上是两种策略

- 一种是Write     Back，写操作只要在cache上，然后再flush到内存上。
- 一种是Write     Through，写操作同时写到cache和内存上。

为了提高写的性能，一般来说，主流的CPU采用的是Write Back的策略，因为直接写内存实在是太慢了。

好了，现在问题来了，如果有一个数据 x 在 CPU 第0核的缓存上被更新了，那么其它CPU核上对于这个数据 x 的值也要被更新，这就是缓存一致性的问题。一般来说，在CPU硬件上，会有两种方法来解决这个问题。

- **Directory** **协议**。这种方法的典型实现是要设计一个集中式控制器，它是主存储器控制器的一部分。其中有一个目录存储在主存储器中，其中包含有关各种本地缓存内容的全局状态信息。当单个CPU     Cache 发出读写请求时，这个集中式控制器会检查并发出必要的命令，以在主存和CPU     Cache之间或在CPU Cache自身之间进行数据同步和传输。
- **Snoopy** **协议**。这种协议更像是一种数据通知的总线型的技术。CPU     Cache通过这个协议可以识别其它Cache上的数据状态。如果有数据共享的话，可以通过广播机制将共享数据的状态通知给其它CPU     Cache。这个协议要求每个CPU Cache 都可以**“窥探”**数据事件的通知并做出相应的反应。如下图所示，有一个Snoopy     Bus的总线。

![image.png](https://i.loli.net/2020/10/08/JDMBGOoaVP6i382.png)

现在的CPU基本用的都是总线式,

![img](https://pic4.zhimg.com/80/v2-d36713b8be816c3fc9cad901bc9c0f3b_1440w.jpg)如图所示，一个变量（一个内存位置）其实可以被多个Cache所共享。那么，当我们需要修改这个变量的时候，Cache要如何保持一致呢？

理想情况下，原子地修改多个Cache，但多个CPU之间往往通过总线进行通信，不可能同时修改多个；所以其实要制造一种假象，看起来是原子地修改多个Cache，也就是让Cache看起来是强一致的。

![img](https://pic1.zhimg.com/80/v2-f75bd8f7006006b2fb8b587a81f68d04_1440w.jpg)

基于总线通信去实现Cache的强一致，这个问题比较明确，目前用的比较多的应该是MESI协议，或者是一些优化的协议。基本思想是这样子的：一个Cache加载一个变量的时候，是Exclusive状态，当这个变量被第二个Cache加载，更改状态为Shared；这时候一个CPU要修改变量， 就把状态改为Modified，并且Invalidate其他的Cache，其他的Cache再去读这个变量，达到一致。MESI协议大致是这样子，但是状态转换要比这个复杂的多。

![img](https://pic2.zhimg.com/80/v2-f11ec6e6b0f9d77f3653a42045bbb911_1440w.jpg)

看起来很美好的MESI协议，其实有一些问题。比如说，修改变量的时候，要发送一些Invalidate给远程的CPU，等到远程CPU返回一个ACK，才能进行下一步。 这一过程中如果远程的CPU比较繁忙，甚至会带来更大的延迟。并且如果有内存访问，会带来几百个周期的延迟。

那么有没有优化手段，能够并行访问内存？或者对内存操作乱序执行？

![img](https://pic2.zhimg.com/80/v2-be68901e3f48768d3c911c2c36bdf109_1440w.jpg)

但是有一个问题我们显然不能忽视，例如这里的例子。程序的正确性依赖了一个假定，x = 1024 这个语句要先于 flag = true 执行。如果这个顺序被破坏了，那么后面的断言就会出错了。

![img](https://pic3.zhimg.com/80/v2-890f9aeb259b5b772f33bfa5cb8a3472_1440w.jpg)

不过先不管这么多，我们先优化一下性能再说。

这里用了一个称之为store buffer的结构，来对store操作进行优化。

![img](https://pic4.zhimg.com/80/v2-69bcbdf56867531799fc699544a1192b_1440w.jpg)

以及Invalidate Queue的结构，可以缓解大量的Invalidate Message的问题。

就Store操作来说，这两个结构所带来的效果就是，不需要等到Cache同步到所有CPU之后Store操作才返回，可能是写了本地的Store buffer就返回，或者invalidate message发到远程CPU，但是远程CPU还没有执行，本地的Store操作也可以返回。显然，这两个结果对于延迟的优化是十分明显的。

![img](https://pic4.zhimg.com/80/v2-1182cc5a55b6d59e2b5c29ab7329fe2b_1440w.jpg)

但是这两个结构肯定会带来乱序的问题，也就是说，本地的Store操作返回了，但其实远程还不能读到，还没有生效，而后面的操作先执行了。如何解决乱序的问题？

CPU通常提供了内存屏障指令，来解决这样的问题。读屏障，清空本地的invalidate queue，保证之前的所有load都已经生效；写屏障，清空本地的store buffer，使得之前的所有store操作都生效。

这里也解释了让我困惑许久的问题，为什么原子变量load和store要配对使用，仅仅store为什么还不够。

![img](https://pic2.zhimg.com/80/v2-e56159f3333000d8cece6bacbaf5e725_1440w.jpg)

使用这两个内存屏障，我们可以对之前的代码做一些修改，来保证正确。

![img](https://pic1.zhimg.com/80/v2-7a1f85bc13f2b526ea3175d3bca6156c_1440w.jpg)

但是，处理器领域其实还有很多的优化手段，流水线执行、乱序执行、预测执行等等，各种我听过和没听过的优化，他们对顺序的影响又是怎样的？以及，我们所说的内存屏障，能否通过形式化方法证明其正确性呢？而不是拘泥于某一个处理器的实现细节，去讨论程序的正确与否，否则这样给程序员带来的心智负担就太重了。

![img](https://pic1.zhimg.com/80/v2-f5299306348a21ae5a740780ec7e0a3c_1440w.jpg)

这里列举了一下主流的CPU架构对指令重排的约定。可以关注一下我们用的比较多的x86，它其实只有一种重排，Store-Load，其实这种称之为Total Store Ordering。除此之外还有很多中处理器，对于重排的定义都不太一样，最弱的Alpha，所有重排都会发生，这种情况下我们的代码要怎么写呢？

![img](https://pic4.zhimg.com/80/v2-12d1f2e228e5e63d8cc03cef9e136227_1440w.jpg)

因此便有了内存模型（Memory Model），它是系统和程序员之间的规范，它规定了存储器访问的行为，并影响到了性能。并且，Memory Model有多层，处理器规定、编译器规定、高级语。对于高级语言来说， 它通常需要支持跨平台，也就是说它会基于各种不同的内存模型，但是又要提供给程序员一个统一的内存模型，可以理解为一个适配器的角色。

Memory Model由Instruction Reordering和Store Atomicity来定义。总是就是每种模型对于乱序的定义不太一样。

![img](https://pic3.zhimg.com/80/v2-ec0ed44dead2c4537dca98abe47629e2_1440w.jpg)

这里其实就有了Memory Consistency的概念，与Cache Coherence不同的是，Memory Consistency关注的是多个变量，而非单个变量；Memory Model是多处理器和编译器优化导致存储器操作被多个处理器观察到的顺序不一致的问题，而Cache Coherence对程序员来说是透明的。

从强到弱，分别是Sequential Consistency，Weak Consistency等等。

![img](https://pic2.zhimg.com/80/v2-165d5c3dc74907ebdf553d2db18b2e05_1440w.jpg)

在CPU这个层面，我们往往不讨论Linearizability，但是在编程的时候通常会把它作为衡量程序正确性的标志，所谓的线程安全/并发安全，通常指的也就是Linearizability。

补充：并发安全和线性一致并不等价，也有非线性一致但并发安全的对象，比如静态一致顺序一致，在某些特殊的场景下不需要保证线性一致也能保证安全。下图引用自《多处理器编程艺术》3.5.2节。

![img](https://pic3.zhimg.com/80/v2-7df773ed4c228844928bedaff2d5506a_1440w.jpg)



![img](https://pic2.zhimg.com/80/v2-b71ebcf8af3e3371d198c2e485250bed_1440w.jpg)

对于Memory Order来说，最重要的是Sequential Consistency。它的意思是，每个线程按照程序次序执行，多个线程的执行结果，和将所有操作顺序执行的结果一样。换句话说，将所有线程的操作按照某一个顺序依次执行，结果和原来一样，但这个顺序未必是时间顺序。

硬件通常也不能保证SC，因为这样性能太差了；但是高级语言通过一些同步手段，通常能保证SC。

现实中的处理器，往往是Relaxed Consistency的，存在着各种乱序，实现为Release consisteny/Processor consistency/Weak Ordering等各种内存模型。

![img](https://pic4.zhimg.com/80/v2-cb7d30bb2a4d516c09c0b33ecc5b479b_1440w.jpg)

总而言之，多处理器编程所要解决的问题，就是基于一个Relaxed Consistency的处理器，通过编程语言和CPU提供的能保证Sequential Consistency的同步原语，最终实现Linearizability。



### Java中的缓存知识

Cache Line伪共享及解决方案

Cache Line伪共享分析

说伪共享前，先看看Cache Line 在java编程中使用的场景。如果CPU访问的内存数据不在Cache中（一级、二级、三级），这就产生了Cache Line miss问题，此时CPU不得不发出新的加载指令，从内存中获取数据。而从内存中访问数据就会产生一个较大的时延，程序性能显著降低。为此我们不得不提高Cache命中率，也就是充分发挥局部性原理。

局部性包括时间局部性、空间局部性。时间局部性：对于同一数据可能被多次使用，自第一次加载到Cache Line后，后面的访问就可以多次从Cache Line中命中，从而提高读取速度（而不是从下层缓存读取）。空间局部性：一个Cache Line有64字节块，我们可以充分利用一次加载64字节的空间，把程序后续会访问的数据，一次性全部加载进来，从而提高Cache Line命中率（而不是重新去寻址读取）。

看个例子：内存地址是连续的数组（利用空间局部性），能一次被L1缓存加载完成。

如下代码，长度为16的row和column数组，在Cache Line 64字节数据块上内存地址是连续的，能被一次加载到Cache Line中，所以在访问数组时，Cache Line命中率高，性能发挥到极致。

```
 
public int run(int[] row, int[] column) {
    int sum = 0;
    for (int i = 0; i < 16; i++) {
      sum += row[i] * column[i];
    }
    return sum;
  }
```

 

而上面例子中变量i则体现了时间局部性，i作为计数器被频繁操作，一直存放在寄存器中，每次从寄存器访问，而不是从主存甚至磁盘访问。虽然连续紧凑的内存分配带来高性能，但并不代表它一直都能带来高性能。如果把它放在多线程中将会发生什么呢？如图：

![image](http://tva4.sinaimg.cn/large/008a4NWwgy1gjic8y10ekj30e30c4dgf.jpg)

数据X、Y、Z被加载到同一Cache Line中，线程A在Core1修改X，线程B在Core2上修改Y。根据MESI大法，假设是Core1是第一个发起操作的CPU核，Core1上的L1 Cache Line由S（共享）状态变成M（修改，脏数据）状态，然后告知其他的CPU核，图例则是Core2，引用同一地址的Cache Line已经无效了；当Core2发起写操作时，首先导致Core1将X写回主存，Cache Line状态由M变为I（无效），而后才是Core2从主存重新读取该地址内容，Cache Line状态由I变成E（独占），最后进行修改Y操作， Cache Line从E变成M。可见多个线程操作在同一Cache Line上的不同数据，相互竞争同一Cache Line，导致线程彼此牵制影响，变成了串行程序，降低了并发性。此时我们则需要将共享在多线程间的数据进行隔离，使他们不在同一个Cache Line上，从而提升多线程的性能。

Cache Line伪共享处理方案

处理伪共享的两种方式：

1. 增大数组元素的间隔使得不同线程存取的元素位于不同的cache     line上。典型的空间换时间。（Linux cache机制与之相关）
2. 在每个线程中创建全局数组各个元素的本地拷贝，然后结束后再写回全局数组。

在Java类中，最优化的设计是考虑清楚哪些变量是不变的，哪些是经常变化的，哪些变化是完全相互独立的，哪些属性一起变化。举个例子：

 

```
public class Data{
  long modifyTime;
  boolean flag;
  long createTime;
  char key;
  int value;
}
 
```

假如业务场景中，上述的类满足以下几个特点：

1. 当value变量改变时，modifyTime肯定会改变
2. createTime变量和key变量在创建后，就不会再变化。
3. flag也经常会变化，不过与modifyTime和value变量毫无关联。

当上面的对象需要由多个线程同时的访问时，从Cache角度来说，就会有一些有趣的问题。当我们没有加任何措施时，Data对象所有的变量极有可能被加载在L1缓存的一行Cache Line中。在高并发访问下，会出现这种问题：

![image](http://tva2.sinaimg.cn/large/008a4NWwgy1gjica6huy6j30fk08jmxi.jpg)

如上图所示，每次value变更时，根据MESI协议，对象其他CPU上相关的Cache Line全部被设置为失效。其他的处理器想要访问未变化的数据(key 和 createTime)时，必须从内存中重新拉取数据，增大了数据访问的开销。

Padding 方式

正确的方式应该将该对象属性分组，将一起变化的放在一组，与其他属性无关的属性放到一组，将不变的属性放到一组。这样当每次对象变化时，不会带动所有的属性重新加载缓存，提升了读取效率。在JDK1.8以前，我们一般是在属性间增加长整型变量来分隔每一组属性。被操作的每一组属性占的字节数加上前后填充属性所占的字节数，不小于一个cache line的字节数就可以达到要求：

 

```
public class DataPadding{
  long a1,a2,a3,a4,a5,a6,a7,a8;//防止与前一个对象产生伪共享
  int value;
  long modifyTime;
  long b1,b2,b3,b4,b5,b6,b7,b8;//防止不相关变量伪共享;
  boolean flag;
  long c1,c2,c3,c4,c5,c6,c7,c8;//
  long createTime;
  char key;
  long d1,d2,d3,d4,d5,d6,d7,d8;//防止与下一个对象产生伪共享
}
```

通过填充变量，使不相关的变量分开

Contended注解方式

在JDK1.8中，新增了一种注解@sun.misc.Contended，来使各个变量在Cache line中分隔开。注意，jvm需要添加参数-XX:-RestrictContended才能开启此功能 

用时，可以在类前或属性前加上此注释：

```
// 类前加上代表整个类的每个变量都会在单独的cache line中
@sun.misc.Contended
@SuppressWarnings("restriction")
public class ContendedData {
  int value;
  long modifyTime;
  boolean flag;
  long createTime;
  char key;
}
 
或者这种：
// 属性前加上时需要加上组标签
@SuppressWarnings("restriction")
public class ContendedGroupData {
  @sun.misc.Contended("group1")
  int value;
 
  @sun.misc.Contended("group1")
  long modifyTime;
 
  @sun.misc.Contended("group2")
  boolean flag;
 
  @sun.misc.Contended("group3")
  long createTime;
 
  @sun.misc.Contended("group3")
  char key;
}
 
 
```

采取上述措施图示：

![image](http://tva3.sinaimg.cn/large/008a4NWwgy1gjicaler39j30g408w762.jpg)

JDK1.8 ConcurrentHashMap的处理

java.util.concurrent.ConcurrentHashMap在这个如雷贯耳的Map中，有一个很基本的操作问题，在并发条件下进行++操作。因为++这个操作并不是原子的，而且在连续的Atomic中，很容易产生伪共享（false sharing）。所以在其内部有专门的数据结构来保存long型的数据:

```
（openjdk\jdk\src\share\classes\java\util\concurrent\ConcurrentHashMap.java line:2506）：
/* ---------------- Counter support -------------- */
/**
* A padded cell for distributing counts. Adapted from LongAdder
* and Striped64. See their internal docs for explanation.
*/
@sun.misc.Contended 
static final class CounterCell {
  volatile long value;
  CounterCell(long x) { 
    value = x; 
  }
}
```

我们看到该类中，是通过@sun.misc.Contended达到防止false sharing的目的

JDK1.8 Thread 的处理

java.lang.Thread在java中，生成随机数是和线程有着关联。而且在很多情况下，多线程下产生随机数的操作是很常见的，JDK为了确保产生随机数的操作不会产生false sharing ,把产生随机数的三个相关值设为独占cache line。

```
@sun.misc.Contended("tlr")
long threadLocalRandomSeed;
 
@sun.misc.Contended("tlr")
 
int threadLocalRandomProbe;
 
@sun.misc.Contended("tlr")
int threadLocalRandomSecondarySeed;
```

Java中对Cache line经典设计

Disruptor框架

**认识Disruptor**

Disruptor是一个线程内通信框架，用于线程里共享数据。与LinkedBlockingQueue类似，提供了一个高速的生产者消费者模型，广泛用于批量IO读写，在硬盘读写相关的程序中应用的十分广泛，Apache旗下的HBase、Hive、Storm等框架都有在使用Disruptor。LMAX 创建Disruptor作为可靠消息架构的一部分，并将它设计成一种在不同组件中共享数据非常快的方法。Disruptor运行大致流程入下图：

![image](http://tva2.sinaimg.cn/large/008a4NWwgy1gjicavh62rj30dx0c7n3k.jpg)

图中左侧（Input Disruptor部分）可以看作多生产者单消费者模式。外部多个线程作为多生产者并发请求业务逻辑处理器（Business Logic Processor），这些请求的信息经过Receiver存放在粉红色的圆环中，业务处理器则作为消费者从圆环中取得数据进行处理。右侧（Output Disruptor部分）则可看作单生产者多消费者模式。业务逻辑处理器作为单生产者，发布数据到粉红色圆环中，Publisher作为多个消费者接受业务逻辑处理器的结果。这里两处地方的数据共享都是通过那个粉红色的圆环，它就是Disruptor的核心设计RingBuffer。

**Disruptor特点**

1. 无锁机制。
2. 没有CAS操作，避免了内存屏障指令的耗时。
3. 避开了Cache     line伪共享的问题，也是Disruptor部分主要关注的主题。

Disruptor对伪共享的处理

**RingBuffer类**

RingBuffer类（即上节中粉红色的圆环）的类关系图如下：

![image](http://tva1.sinaimg.cn/large/008a4NWwgy1gjicb0ilu9j30f606i74e.jpg)

通过源码分析，RingBuffer的父类，RingBufferFields采用数组来实现存放线程间的共享数据。下图，第57行，entries数组。

![image](http://tva3.sinaimg.cn/large/008a4NWwgy1gjicb6m88wj30by068t8t.jpg)

前面分析过数组比链表、树更具有缓存友好性，此处不做细表。不使用LinkedBlockingQueue队列，是基于无锁机制的考虑。详细分析可参考，并发编程网的翻译。这里我们主要分析RingBuffer的继承关系中的填充，解决缓存伪共享问题。如下图： 

![image](http://tvax3.sinaimg.cn/large/008a4NWwgy1gjicbfivadj30fl04zdfv.jpg)

![image](http://tvax2.sinaimg.cn/large/008a4NWwgy1gjicbjgq3xj30rc06dt96.jpg)

依据JVM对象继承关系中父类属性与子类属性，内存地址连续排列布局，RingBufferPad的protected long p1,p2,p3,p4,p5,p6,p7;作为缓存前置填充，RingBuffer中的protected long p1,p2,p3,p4,p5,p6,p7;作为缓存后置填充。这样任意线程访问RingBuffer时，RingBuffer放在父类RingBufferFields的属性，都是独占一行Cache line不会产生伪共享问题。如图，RingBuffer的操作字段在RingBufferFields中，使用rbf标识：

![image](http://tva4.sinaimg.cn/large/008a4NWwgy1gjicbqivwvj30rc0g4tdf.jpg)

按照一行缓存64字节计算，前后填充56字节（7个long），中间大于等于8字节的内容都能独占一行Cache line，此处rbf是大于8字节的。

**Sequence类**

Sequence类用来跟踪RingBuffer和事件处理器的增长步数，支持多个并发操作包括CAS指令和写指令。同时使用了Padding方式来实现，如下为其类结构图及Padding的类。

Sequence里在volatile long value前后放置了7个long padding，来解决伪共享的问题。示意如图，此处Value等于8字节：

也许读者应该会认为这里的图示比上面RingBuffer的图示更好理解，这里的操作属性只有一个value，两个图相互结合就更能理解了。

**Sequencer的实现**

在RingBuffer构造函数里面存在一个Sequencer接口，用来遍历数据，在生产者和消费者之间传递数据。Sequencer有两个实现类，单生产者模式的实现SingleProducerSequencer与多生产者模式的实现MultiProducerSequencer。它们的类结构如图：

单生产者是在Cache line中使用padding方式实现，源码如下：![image](http://tvax1.sinaimg.cn/large/008a4NWwgy1gjicbyw6nxj30fc0b10up.jpg)

多生产者则是使用 sun.misc.Unsafe来实现的。如下图：

![image](http://tva3.sinaimg.cn/large/008a4NWwgy1gjicc5p2w2j30fw03umxv.jpg)